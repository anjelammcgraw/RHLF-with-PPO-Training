# ü§ñ RLHF: Reinforcement from Human Feedback In Practice

In this repo, we'll be working toward reducing toxicity in a base model. The base model we'll be using is the Zephyr-7b-alpha model! This project was done during Week 4 of AI Makerspace's LLM Engineering Cohort 2 (LLME2).

### ‚öôÔ∏èThe colab links to the code are found below and will also be included in this repo 

### ü´ÇThe video walkthrough can be found [here.](https://www.loom.com/share/02c765ae53b143b1a4b5fb0f3699aa69?sid=21f448d3-4775-47af-90ca-5ab05a29d37f)

# ‚öôÔ∏èThe Build Process

### Evaluating Our Base Model
The colab link to evaluating our base model is found [here.](https://colab.research.google.com/drive/1xkuO1jO9qZBdHsvGbRccqSpX_YN8qVCk?usp=sharing)

### Reward Model & PPO Training
The colab link to our Reward Model PPO Training is found [here.](https://colab.research.google.com/drive/1TMiJHfeLiwvt2ET33NeaPJgJpAtOXcL1?usp=sharing)

